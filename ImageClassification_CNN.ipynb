{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an image classification model using very little data\n",
    "Based on the tutorial by Francois Chollet @fchollet https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html and the workbook by Guillaume Dominici https://github.com/gggdominici/keras-workshop\n",
    "\n",
    "This tutorial presents several ways to build an image classifier using keras from just a few hundred or thousand pictures from each class you want to be able to recognize.\n",
    "\n",
    "We will go over the following options:\n",
    " * training a small network from scratch (as a baseline)\n",
    " * using the bottleneck features of a pre-trained network\n",
    " * fine-tuning the top layers of a pre-trained network\n",
    "\n",
    "This will lead us to cover the following Keras features:\n",
    "\n",
    "* fit_generator for training Keras a model using Python data generators\n",
    "* ImageDataGenerator for real-time data augmentation\n",
    "* layer freezing and model fine-tuning\n",
    "* ...and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /Users/z00193k/anaconda/lib/python2.7/site-packages\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##This notebook is built around using tensorflow as the backend for keras\n",
    "!pip install pillow\n",
    "!KERAS_BACKEND=tensorflow python -c \"from keras import backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##Updated to Keras 2.0\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras import optimizers\n",
    "from keras import applications\n",
    "from keras.models import Model\n",
    "\n",
    "os.chdir('/Users/z00193k/Desktop/ML_AI_DL/CNN/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2048 images belonging to 2 classes.\n",
      "Found 824 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "##preprocessing\n",
    "# used to rescale the pixel values from [0, 255] to [0, 1] interval\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 32\n",
    "\n",
    "# automagically retrieve images and their classes for train and validation sets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Conv Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a simple stack of 3 convolution layers with a ReLU activation and followed by max-pooling layers.\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, (3, 3), input_shape=(img_width, img_height,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "train_samples = 2048\n",
    "validation_samples = 832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "64/64 [==============================] - 50s - loss: 0.6199 - acc: 0.6582 - val_loss: 0.5838 - val_acc: 0.7027\n",
      "Epoch 2/30\n",
      "64/64 [==============================] - 57s - loss: 0.5864 - acc: 0.6948 - val_loss: 0.5901 - val_acc: 0.6663\n",
      "Epoch 3/30\n",
      "64/64 [==============================] - 61s - loss: 0.5156 - acc: 0.7407 - val_loss: 0.5731 - val_acc: 0.6881\n",
      "Epoch 4/30\n",
      "64/64 [==============================] - 52s - loss: 0.4990 - acc: 0.7588 - val_loss: 0.6906 - val_acc: 0.6626\n",
      "Epoch 5/30\n",
      "64/64 [==============================] - 54s - loss: 0.4532 - acc: 0.7817 - val_loss: 0.5914 - val_acc: 0.7087\n",
      "Epoch 6/30\n",
      "64/64 [==============================] - 53s - loss: 0.4169 - acc: 0.8042 - val_loss: 0.6900 - val_acc: 0.6942\n",
      "Epoch 7/30\n",
      "64/64 [==============================] - 53s - loss: 0.3887 - acc: 0.8267 - val_loss: 0.5766 - val_acc: 0.7318\n",
      "Epoch 8/30\n",
      "64/64 [==============================] - 53s - loss: 0.3388 - acc: 0.8540 - val_loss: 0.6191 - val_acc: 0.7439\n",
      "Epoch 9/30\n",
      "64/64 [==============================] - 54s - loss: 0.3000 - acc: 0.8677 - val_loss: 0.7683 - val_acc: 0.7197\n",
      "Epoch 10/30\n",
      "64/64 [==============================] - 56s - loss: 0.2595 - acc: 0.8853 - val_loss: 0.7622 - val_acc: 0.7002\n",
      "Epoch 11/30\n",
      "64/64 [==============================] - 55s - loss: 0.2297 - acc: 0.9038 - val_loss: 0.7737 - val_acc: 0.7136\n",
      "Epoch 12/30\n",
      "64/64 [==============================] - 55s - loss: 0.2015 - acc: 0.9150 - val_loss: 0.7137 - val_acc: 0.7488\n",
      "Epoch 13/30\n",
      "64/64 [==============================] - 55s - loss: 0.1753 - acc: 0.9307 - val_loss: 0.9000 - val_acc: 0.7306\n",
      "Epoch 14/30\n",
      "64/64 [==============================] - 56s - loss: 0.1516 - acc: 0.9414 - val_loss: 1.3547 - val_acc: 0.6869\n",
      "Epoch 15/30\n",
      "64/64 [==============================] - 56s - loss: 0.1315 - acc: 0.9541 - val_loss: 0.8895 - val_acc: 0.7318\n",
      "Epoch 16/30\n",
      "64/64 [==============================] - 56s - loss: 0.1168 - acc: 0.9565 - val_loss: 1.0507 - val_acc: 0.7476\n",
      "Epoch 17/30\n",
      "64/64 [==============================] - 56s - loss: 0.1003 - acc: 0.9624 - val_loss: 1.0611 - val_acc: 0.6905\n",
      "Epoch 18/30\n",
      "64/64 [==============================] - 56s - loss: 0.0931 - acc: 0.9644 - val_loss: 1.6490 - val_acc: 0.6845\n",
      "Epoch 19/30\n",
      "64/64 [==============================] - 56s - loss: 0.0901 - acc: 0.9697 - val_loss: 1.0657 - val_acc: 0.7197\n",
      "Epoch 20/30\n",
      "64/64 [==============================] - 56s - loss: 0.0706 - acc: 0.9741 - val_loss: 1.4018 - val_acc: 0.7282\n",
      "Epoch 21/30\n",
      "64/64 [==============================] - 56s - loss: 0.0756 - acc: 0.9727 - val_loss: 1.5966 - val_acc: 0.7233\n",
      "Epoch 22/30\n",
      "64/64 [==============================] - 57s - loss: 0.0641 - acc: 0.9795 - val_loss: 1.4710 - val_acc: 0.7245\n",
      "Epoch 23/30\n",
      "64/64 [==============================] - 56s - loss: 0.0699 - acc: 0.9756 - val_loss: 1.5999 - val_acc: 0.7245\n",
      "Epoch 24/30\n",
      "64/64 [==============================] - 55s - loss: 0.0620 - acc: 0.9795 - val_loss: 1.2341 - val_acc: 0.7172\n",
      "Epoch 25/30\n",
      "64/64 [==============================] - 55s - loss: 0.0739 - acc: 0.9766 - val_loss: 1.7382 - val_acc: 0.7269\n",
      "Epoch 26/30\n",
      "64/64 [==============================] - 55s - loss: 0.0501 - acc: 0.9814 - val_loss: 1.9019 - val_acc: 0.7439\n",
      "Epoch 27/30\n",
      "64/64 [==============================] - 55s - loss: 0.0860 - acc: 0.9741 - val_loss: 1.9461 - val_acc: 0.7124\n",
      "Epoch 28/30\n",
      "64/64 [==============================] - 56s - loss: 0.0623 - acc: 0.9766 - val_loss: 1.7739 - val_acc: 0.7294\n",
      "Epoch 29/30\n",
      "64/64 [==============================] - 55s - loss: 0.1101 - acc: 0.9653 - val_loss: 1.3522 - val_acc: 0.7379\n",
      "Epoch 30/30\n",
      "64/64 [==============================] - 55s - loss: 0.0620 - acc: 0.9824 - val_loss: 1.8586 - val_acc: 0.7100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x120bcefd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=train_samples // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_samples// batch_size,)\n",
    "#About 60 seconds an epoch when using CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('models/basic_cnn_1_epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.8083927611124169, 0.71962226939939178]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(validation_generator, validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
